{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/trf1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/trf1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "warnings.filterwarnings('ignore') # remove warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "raw_test_data = pd.read_csv(r\"test.csv\").drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95b1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "\n",
    "random.seed(25) # set a random seed\n",
    "copy_raw_training_data = raw_training_data.copy()  # copy the raw training data\n",
    "shape = copy_raw_training_data.shape  # store dimension of raw training data\n",
    "num_entries = shape[0] * (shape[1] - 1)  # store number of entries for all independent variables\n",
    "num_null = int(num_entries * 0.01)  # make 1% of the data be null entries\n",
    "\n",
    "# randomly select entries and replace them with NaN, excluding the response variable\n",
    "for _ in range(num_null):\n",
    "    rand_row, rand_col = random.randint(0, shape[0] - 1) , random.randint(0, shape[1] - 2) # select a random row and column\n",
    "    copy_raw_training_data.iloc[rand_row, rand_col] = np.nan # store entry as nan\n",
    "    \n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(copy_raw_training_data.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "# store all of the entries for the original data set in the positions of the removed entries\n",
    "actual_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "for row, col in null_entries: # loop over every removed entry:\n",
    "    col_name = copy_raw_training_data.columns[col] # store column name of current null entry\n",
    "    if col_name not in actual_entries_store: # check if column name is already in the dictionary\n",
    "        actual_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "    actual_entries_store[col_name].append(raw_training_data.iloc[row, col])# add actual value to the list for correct column\n",
    "    \n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in copy_raw_training_data.select_dtypes(include=['object']).columns.tolist() if col != 'Status']\n",
    "num_cols = [x for x in copy_raw_training_data.columns.drop(['Status']) if x not in categorical_cols]\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# function to calculate performance metrics for numerical variables\n",
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": round(mean_squared_error(actual_vals, imputed_vals, squared=False),3)}) # add the column name and its RMSE\n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)\n",
    "\n",
    "# function to calculate performance metrics for categorical variables\n",
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": round(accuracy,3), \"F1 Score\": round(f1,3)}) # add the column name and its accuracy and F1 score\n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1178.102\n",
      "    Bilirubin    4.242\n",
      "  Prothrombin    0.796\n",
      "          Age 3782.324\n",
      "  Cholesterol  129.944\n",
      "      Albumin    0.349\n",
      "       Copper   50.941\n",
      "     Alk_Phos 2026.417\n",
      "    Platelets   83.103\n",
      "         SGOT   42.677\n",
      "Tryglicerides   76.562\n",
      "        Stage    0.897\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.570     0.414\n",
      "         Sex     0.920     0.882\n",
      "     Ascites     0.949     0.925\n",
      "Hepatomegaly     0.549     0.389\n",
      "     Spiders     0.809     0.723\n",
      "       Edema     0.918     0.878\n"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in copy_raw_training_data.columns.drop(['Status']): # loop over each column of the data set \n",
    "    col_data = copy_raw_training_data[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries if col_name == copy_raw_training_data.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_store, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_store, imputed1_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aa663cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N_Days           float64\n",
       "Drug              object\n",
       "Age              float64\n",
       "Sex               object\n",
       "Ascites           object\n",
       "Hepatomegaly      object\n",
       "Spiders           object\n",
       "Edema             object\n",
       "Bilirubin        float64\n",
       "Cholesterol      float64\n",
       "Albumin          float64\n",
       "Copper           float64\n",
       "Alk_Phos         float64\n",
       "SGOT             float64\n",
       "Tryglicerides    float64\n",
       "Platelets        float64\n",
       "Prothrombin      float64\n",
       "Stage            float64\n",
       "Status            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_raw_training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f99360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8310e+03, 2.1532e+04, 2.3000e+00, ..., 3.9400e+02, 9.7000e+00,\n",
       "        3.0000e+00],\n",
       "       [2.5740e+03, 1.9237e+04, 1.1000e+00, ..., 3.6100e+02, 1.1000e+01,\n",
       "        3.0000e+00],\n",
       "       [3.4280e+03, 1.3727e+04, 3.3000e+00, ..., 1.9900e+02, 1.1700e+01,\n",
       "        4.0000e+00],\n",
       "       ...,\n",
       "       [1.5760e+03, 2.5873e+04, 2.0000e+00, ..., 2.0000e+02, 1.2700e+01,\n",
       "        2.0000e+00],\n",
       "       [3.5840e+03, 2.2960e+04, 7.0000e-01, ..., 2.2100e+02, 1.0600e+01,\n",
       "        4.0000e+00],\n",
       "       [1.9780e+03, 1.9237e+04, 7.0000e-01, ..., 3.3600e+02, 1.0300e+01,\n",
       "        3.0000e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation 2: impute each missing value with the mean value from k nearest neighbours\n",
    "\n",
    "independent_vars = copy_raw_training_data.drop(labels=['Status'], axis=1)   # ignore the status column\n",
    "numerical = independent_vars.select_dtypes(include=[\"float64\", \"int64\"])    # separate out the numerical data\n",
    "categorical = independent_vars.select_dtypes(exclude=[\"float64\", \"int64\"])\n",
    "\n",
    "imputed2_entries_store = {} # create empty dictionary\n",
    "imputer2 = KNNImputer(n_neighbors=2)    # create a KNN imputer\n",
    "imputer2.fit_transform(numerical)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
