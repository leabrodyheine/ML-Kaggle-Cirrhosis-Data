{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/trf1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/trf1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore') # remove warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "raw_test_data = pd.read_csv(r\"test.csv\").drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c95b1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "\n",
    "random.seed(25) # set a random seed\n",
    "copy_raw_training_data = raw_training_data.copy()  # copy the raw training data\n",
    "shape = copy_raw_training_data.shape  # store dimension of raw training data\n",
    "num_entries = shape[0] * (shape[1] - 1)  # store number of entries for all independent variables\n",
    "num_null = int(num_entries * 0.01)  # make 1% of the data be null entries\n",
    "\n",
    "# randomly select entries and replace them with NaN, excluding the response variable\n",
    "for _ in range(num_null):\n",
    "    rand_row, rand_col = random.randint(0, shape[0] - 1) , random.randint(0, shape[1] - 2) # select a random row and column\n",
    "    copy_raw_training_data.iloc[rand_row, rand_col] = np.nan # store entry as nan\n",
    "    \n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(copy_raw_training_data.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "# store all of the entries for the original data set in the positions of the removed entries\n",
    "actual_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "for row, col in null_entries: # loop over every removed entry:\n",
    "    col_name = copy_raw_training_data.columns[col] # store column name of current null entry\n",
    "    if col_name not in actual_entries_store: # check if column name is already in the dictionary\n",
    "        actual_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "    actual_entries_store[col_name].append(raw_training_data.iloc[row, col])# add actual value to the list for correct column\n",
    "    \n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in copy_raw_training_data.select_dtypes(include=['object']).columns.tolist() if col != 'Status']\n",
    "num_cols = [x for x in copy_raw_training_data.columns.drop(['Status']) if x not in categorical_cols]\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# function to calculate performance metrics for numerical variables\n",
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": mean_squared_error(actual_vals, imputed_vals, squared=False).round(3)}) # add the column name and its RMSE\n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)\n",
    "\n",
    "# function to calculate performance metrics for categorical variables\n",
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": accuracy.round(3), \"F1 Score\": f1.round(3)}) # add the column name and its accuracy and F1 score\n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1178.102\n",
      "    Bilirubin    4.242\n",
      "  Prothrombin    0.796\n",
      "          Age 3782.324\n",
      "  Cholesterol  129.944\n",
      "      Albumin    0.349\n",
      "       Copper   50.941\n",
      "     Alk_Phos 2026.417\n",
      "    Platelets   83.103\n",
      "         SGOT   42.677\n",
      "Tryglicerides   76.562\n",
      "        Stage    0.897\n",
      "\n",
      "Categorical variables performance for imputation 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(calc_num_metrics(actual_entries_store, imputed1_entries_store, num_cols))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCategorical variables performance for imputation 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcalc_categorical_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_entries_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimputed1_entries_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 75\u001b[0m, in \u001b[0;36mcalc_categorical_metrics\u001b[0;34m(actual_entries_store, imputed_entries_store, categorical_cols)\u001b[0m\n\u001b[1;32m     73\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(actual_vals, imputed_vals) \u001b[38;5;66;03m# calculate accuracy score\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(actual_vals, imputed_vals, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# calculate F1 score\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     categorical_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m: col_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43maccuracy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m(\u001b[38;5;241m3\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m)}) \u001b[38;5;66;03m# add the column name and its accuracy and F1 score\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(categorical_data)\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in copy_raw_training_data.columns.drop(['Status']): # loop over each column of the data set \n",
    "    col_data = copy_raw_training_data[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries if col_name == copy_raw_training_data.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_store, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_store, imputed1_entries_store, categorical_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
