{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore') # remove warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "raw_test_data = pd.read_csv(r\"test.csv\").drop(columns=[\"id\"])\n",
    "\n",
    "copy_raw_training_data = raw_training_data.copy()  # copy the raw training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8945e04",
   "metadata": {},
   "source": [
    "Because many of the imputation methods only work on numerical data, we need to encode the categorical variables in our dataframe in some way. We chose to encode them with an integer for each category, since they all had very few options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95b1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(25) # set a random seed\n",
    "copy_raw_training_data = raw_training_data.copy()  # copy the raw training data\n",
    "shape = copy_raw_training_data.shape  # store dimension of raw training data\n",
    "num_entries = shape[0] * (shape[1] - 1)  # store number of entries for all independent variables\n",
    "num_null = int(num_entries * 0.01)  # make 1% of the data be null entries\n",
    "\n",
    "# randomly select entries and replace them with NaN, excluding the response variable\n",
    "for _ in range(num_null):\n",
    "    rand_row, rand_col = random.randint(0, shape[0] - 1) , random.randint(0, shape[1] - 2) # select a random row and column\n",
    "    copy_raw_training_data.iloc[rand_row, rand_col] = np.nan # store entry as nan\n",
    "    \n",
    "missing_raw_training_data = copy_raw_training_data.copy()\n",
    "    \n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(copy_raw_training_data.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "# store all of the entries for the original data set in the positions of the removed entries\n",
    "actual_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "for row, col in null_entries: # loop over every removed entry:\n",
    "    col_name = copy_raw_training_data.columns[col] # store column name of current null entry\n",
    "    if col_name not in actual_entries_store: # check if column name is already in the dictionary\n",
    "        actual_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "    actual_entries_store[col_name].append(raw_training_data.iloc[row, col])# add actual value to the list for correct column\n",
    "    \n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in copy_raw_training_data.select_dtypes(include=['object']).columns.tolist() if col != 'Status']\n",
    "num_cols = [x for x in copy_raw_training_data.columns.drop(['Status']) if x not in categorical_cols]\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# function to calculate performance metrics for numerical variables\n",
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": round(mean_squared_error(actual_vals, imputed_vals, squared=False),3)}) # add the column name and its RMSE\n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)\n",
    "\n",
    "# function to calculate performance metrics for categorical variables\n",
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": round(accuracy,3), \"F1 Score\": round(f1,3)}) # add the column name and its accuracy and F1 score\n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111dce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_values(imputed_data, null_entries):\n",
    "    \"\"\"\n",
    "    Gets the value of all the imputed entries and outputs them in a dictionary\n",
    "    \n",
    "    Parameters:\n",
    "    - null_entries: the locations of all the entries which were null before imputation\n",
    "\n",
    "    Returns:\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    \"\"\"\n",
    "    imputed_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "    \n",
    "    for row, col in null_entries: # loop over every removed entry:\n",
    "        col_name = imputed_data.columns[col] # store column name of current null entry\n",
    "        if col_name not in imputed_entries_store: # check if column name is already in the dictionary\n",
    "            imputed_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "        imputed_entries_store[col_name].append(imputed_data.iloc[row, col])# add actual value to the list for correct column\n",
    "\n",
    "    return imputed_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1178.102\n",
      "    Bilirubin    4.242\n",
      "  Prothrombin    0.796\n",
      "          Age 3782.324\n",
      "  Cholesterol  129.944\n",
      "      Albumin    0.349\n",
      "       Copper   50.941\n",
      "     Alk_Phos 2026.417\n",
      "    Platelets   83.103\n",
      "         SGOT   42.677\n",
      "Tryglicerides   76.562\n",
      "        Stage    0.897\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.570     0.414\n",
      "         Sex     0.920     0.882\n",
      "     Ascites     0.949     0.925\n",
      "Hepatomegaly     0.549     0.389\n",
      "     Spiders     0.809     0.723\n",
      "       Edema     0.918     0.878\n"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in copy_raw_training_data.columns.drop(['Status']): # loop over each column of the data set \n",
    "    col_data = copy_raw_training_data[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries if col_name == copy_raw_training_data.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_store, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_store, imputed1_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736e5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how to encode each categorical variable as an integer, and define a reverse of this encoding\n",
    "numerical_encode = {\n",
    "    \"Drug\": {\"Placebo\": 0, \"D-penicillamine\": 1}, \"Sex\": {\"F\": 0, \"M\": 1}, \"Ascites\": {\"N\": 0, \"Y\": 1},\n",
    "    \"Hepatomegaly\": {\"N\": 0, \"Y\": 1}, \"Spiders\": {\"N\": 0, \"Y\": 1}, \"Edema\": {\"N\": 0, \"Y\": 1, \"S\": 2}}\n",
    "\n",
    "reverse_numerical_encode = {feature: {value: key for key, value in encoding.items()}\n",
    "                            for feature, encoding in numerical_encode.items()}\n",
    "\n",
    "# replace entries in categorical columns with specified numbers\n",
    "training_data_encoded = copy_raw_training_data.replace(numerical_encode)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# separate out the independent variables from the target\n",
    "X_copy = training_data_encoded.drop(labels=['Status'], axis=1)\n",
    "Y_copy = training_data_encoded[\"Status\"]\n",
    "\n",
    "def add_missing_values(X_full, Y_full, missing_rate):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and adds in some missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_full: a dataframe containing all columns with independent variables\n",
    "    - Y_full: a dataframe containing the response variable column only\n",
    "    - missing_rate: a float between 0 and 1 which specifies the proportion of lines which should have missing values\n",
    "\n",
    "    Returns:\n",
    "    - X_missing: the dataframe X_full but with some missing values\n",
    "    - Y_missing: an exact copy of Y_full\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "    np.random.seed(24)\n",
    "    \n",
    "    # copy the input dataframes\n",
    "    X_missing = X_full.copy()\n",
    "    y_missing = Y_full.copy()\n",
    "\n",
    "    # create empty dictionary to store values of entries before setting them to nan\n",
    "    actual_entries_store = {}\n",
    "\n",
    "    # remove a percentage of entries in each column at random, specified by missing_rate\n",
    "    for col in X_missing.columns:\n",
    "        index_list = X_missing.sample(frac=missing_rate).index  # sample a proportion of the indices within the column\n",
    "        actual_entries_store[col] = []\n",
    "        for value in index_list:\n",
    "            actual_entries_store[col].append(X_missing.loc[value, col])   # store the actual entries in the dictionary\n",
    "        X_missing.loc[index_list, col] = np.nan # replace the values with nan\n",
    "\n",
    "    return X_missing, y_missing, actual_entries_store\n",
    "\n",
    "# apply the add_missing_values function to our data\n",
    "X_copy_miss, Y_copy_miss, actual_entries_X = add_missing_values(X_copy, Y_copy, 0.05)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries_X = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(X_copy_miss.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac8806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 2: impute each missing value with the mean value from k nearest neighbours\n",
    "\n",
    "imputer2 = KNNImputer(n_neighbors=2, weights=\"uniform\")    # create a KNN imputer\n",
    "data_imputed_knn_X = pd.DataFrame(imputer2.fit_transform(X=X_copy_miss, y=Y_copy_miss), columns=X_copy_miss.columns) # apply the imputer to the data with missing values\n",
    "\n",
    "# add the target back to the result\n",
    "data_imputed_knn = pd.concat([data_imputed_knn_X, Y_copy_miss], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf876bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed2_entries_store = get_imputed_values(data_imputed_knn, null_entries_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e89e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Variable     RMSE\n",
      "       N_Days 1347.275\n",
      "         Drug    0.596\n",
      "          Age 4256.052\n",
      "          Sex    0.320\n",
      "      Ascites    0.232\n",
      " Hepatomegaly    0.630\n",
      "      Spiders    0.565\n",
      "        Edema    0.561\n",
      "    Bilirubin    5.173\n",
      "  Cholesterol  223.777\n",
      "      Albumin    0.416\n",
      "       Copper   87.412\n",
      "     Alk_Phos 2020.361\n",
      "         SGOT   59.019\n",
      "Tryglicerides   68.322\n",
      "    Platelets  112.273\n",
      "  Prothrombin    0.987\n",
      "        Stage    1.162\n"
     ]
    }
   ],
   "source": [
    "knn_metrics = calc_num_metrics(actual_entries_X, imputed2_entries_store, data_imputed_knn.columns)\n",
    "print(knn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ba3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 3:\n",
      "     Variable     RMSE\n",
      "       N_Days 1019.818\n",
      "    Bilirubin    3.600\n",
      "  Prothrombin    0.726\n",
      "          Age 3826.653\n",
      "  Cholesterol  111.272\n",
      "      Albumin    0.321\n",
      "       Copper   43.167\n",
      "     Alk_Phos 1797.119\n",
      "    Platelets   77.763\n",
      "         SGOT   41.895\n",
      "Tryglicerides   68.187\n",
      "        Stage    0.841\n",
      "\n",
      "Numerical variables performance for imputation 3:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.640     0.637\n",
      "         Sex     0.880     0.884\n",
      "     Ascites     0.937     0.918\n",
      "Hepatomegaly     0.829     0.829\n",
      "     Spiders     0.809     0.823\n",
      "       Edema     0.871     0.885\n"
     ]
    }
   ],
   "source": [
    "# Imputation 3: for numerical variables, implement linear regression using the non-null rows to determine the regression lines.\n",
    "# For rows with more 2 or 3 missing values, fill in 1 or 2, respecitvely, entries with the median for that column\n",
    "# For categorical variables, implement a decision tree classifier to impute the missing entries.\n",
    "\n",
    "np.random.seed(24)\n",
    "# Store the missing data frame for only numerical variables. Store another data frame by dropping the null rows.\n",
    "num_missing_raw_training_data = missing_raw_training_data.drop(categorical_cols, axis=1).drop(['Status'], axis=1)\n",
    "linear_regres_df = num_missing_raw_training_data.dropna()\n",
    "\n",
    "# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\n",
    "regressions = {col: {'intercept': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).intercept_,\n",
    "                     'coeffs': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).coef_}\n",
    "               for col in linear_regres_df.columns}\n",
    "\n",
    "# identify the rows with 1 missing value, and more than 1 missing value\n",
    "rows_with_one_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) == 1]\n",
    "rows_with_over1_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) >= 2]\n",
    "\n",
    "# impute missing values using the median of the column in the rows with 2 or more missing values\n",
    "median_vals = missing_raw_training_data.median() # store median value for each column in missing data frame\n",
    "for idx, row in rows_with_over1_missing.iterrows(): # loop over rows with more than 1 missing value:\n",
    "    missing_indices = row.isnull() # store the columns in current row with null entries\n",
    "    fill_indices = np.random.choice(missing_indices[missing_indices].index, missing_indices.sum() - 1, replace=False) # sellect indicies to fill at random\n",
    "    rows_with_over1_missing.loc[idx, fill_indices] = median_vals[fill_indices] #fill these entries using median of the column\n",
    "\n",
    "# combine data frames such that each row only contains 1 missing value\n",
    "new_rows_with_one_missing = pd.concat([rows_with_over1_missing, rows_with_one_missing]).sort_index()\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# impute missing values using linear regression\n",
    "for idx, row in new_rows_with_one_missing.iterrows(): # loop over each row in this new data frame\n",
    "    for col in num_cols: # loop over numerical variables:\n",
    "        if pd.isna(row[col]): # if value in current cell is na:\n",
    "            row[col] = (regressions[col]['coeffs'] * row.dropna()).sum() + regressions[col]['intercept'] # calc the imputed value using linear regression coefficients and intercept\n",
    "\n",
    "# map to convert numerical variables in 'null_entries' to their names\n",
    "mapping = {0: 'N_Days', 2: 'Age', 8: 'Bilirubin', 9: 'Cholesterol', 10: 'Albumin', 11: 'Copper', 12: 'Alk_Phos', 13: 'SGOT', 14: 'Tryglicerides', 15: 'Platelets', 16: 'Prothrombin', 17: 'Stage'}\n",
    "\n",
    "# create dictionary for imputed values to be used in 'calc_num_metrics' function\n",
    "imputed2_entries_store = {col_name: [] for col_name in mapping.values()} # initialise dictionary with values being empty list\n",
    "for row, col in null_entries: # loop over each null entry:\n",
    "    if col in mapping and isinstance(mapping[col], str): # if column variable is a key in mapping and value is a string:\n",
    "        imputed2_entries_store[mapping[col]].append(new_rows_with_one_missing.loc[row, mapping[col]]) # add the imputed value to the corresponding list\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# create the data set with numerical variables imputed using linear regression for method 2 and categorical variables still missing\n",
    "for index, row in new_rows_with_one_missing.iterrows():\n",
    "    for col in num_cols:\n",
    "        missing_raw_training_data.loc[index,col] = new_rows_with_one_missing.loc[index][col]\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Categorical imputation method:\n",
    "# split data set into numerical and categorical sets, dropping status column and all missing rows, and numerically encode\n",
    "dropped_missing_data = missing_raw_training_data.replace(numerical_encode).dropna().drop(columns=['Status'])\n",
    "\n",
    "# train decision tree classifier for each categorical variable, predict the missing values, and impute into original dataset\n",
    "missing_training_data_encoded = missing_raw_training_data.replace(numerical_encode).copy() # new data frame by numerically encoding\n",
    "for cat_var in categorical_cols: # loop over each categorical variable\n",
    "    # define the training data and labels\n",
    "    X = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    y = dropped_missing_data[cat_var]\n",
    "    # fit the tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X, y)\n",
    "    # predict the missing categorical variables and impute into missing_raw_training_data\n",
    "    X_missing = missing_raw_training_data.drop(columns=categorical_cols).drop(columns=['Status'])\n",
    "    missing_predictions = decision_tree.predict(X_missing)\n",
    "    missing_raw_training_data.loc[missing_raw_training_data[cat_var].isnull(), cat_var] = missing_predictions[missing_raw_training_data[cat_var].isnull()]\n",
    "\n",
    "# restore the original categorical entries instead of being numerical\n",
    "missing_raw_training_data = missing_raw_training_data.replace(reverse_numerical_encode)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "imputed3_entries_store = get_imputed_values(missing_raw_training_data, null_entries) # store imputed values in dictionary\n",
    "print(\"Numerical variables performance for imputation 3:\")\n",
    "print(calc_num_metrics(actual_entries_store, imputed3_entries_store, num_cols))\n",
    "print(\"\\nNumerical variables performance for imputation 3:\")\n",
    "print(calc_categorical_metrics(actual_entries_store, imputed3_entries_store, categorical_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
