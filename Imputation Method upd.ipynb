{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167bcedc",
   "metadata": {},
   "source": [
    "### Import Relevent Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/trf1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/trf1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# remove warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11061e76",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "\n",
    "# copy the raw training data\n",
    "copy_raw_training_data = raw_training_data.copy()\n",
    "\n",
    "# separate out the independent variables from the target\n",
    "X_copy = copy_raw_training_data.drop(labels=['Status'], axis=1)\n",
    "Y_copy = copy_raw_training_data[\"Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebdae6",
   "metadata": {},
   "source": [
    "### Define Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd36630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_values(X_full, Y_full, missing_rate):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and adds in some missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_full: a dataframe containing all columns with independent variables\n",
    "    - Y_full: a dataframe containing the response variable column only\n",
    "    - missing_rate: a float between 0 and 1 which specifies the proportion of lines which should have missing values\n",
    "\n",
    "    Returns:\n",
    "    - X_missing: the dataframe X_full but with some missing values\n",
    "    - Y_missing: an exact copy of Y_full\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "    np.random.seed(24)\n",
    "    \n",
    "    # copy the input dataframes\n",
    "    X_missing = X_full.copy()\n",
    "    y_missing = Y_full.copy()\n",
    "\n",
    "    # create empty dictionary to store values of entries before setting them to nan\n",
    "    actual_entries_store = {}\n",
    "\n",
    "    # remove a percentage of entries in each column at random, specified by missing_rate\n",
    "    for col in X_missing.columns:\n",
    "        index_list = X_missing.sample(frac=missing_rate+random.uniform(-0.01, 0.01)).index  # sample a proportion of the indices within the column, with a random component for the rate\n",
    "        actual_entries_store[col] = []\n",
    "        for value in index_list:\n",
    "            actual_entries_store[col].append(X_missing.loc[value, col])   # store the actual entries in the dictionary\n",
    "        X_missing.loc[index_list, col] = np.nan # replace the values with nan\n",
    "\n",
    "    return X_missing, y_missing, actual_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574af618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_values(imputed_data, null_entries):\n",
    "    \"\"\"\n",
    "    Gets the value of all the imputed entries and outputs them in a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - imputed_data: the dataframe containing some actual values and some imputed values\n",
    "    - null_entries: the locations of all the entries which were null before imputation\n",
    "\n",
    "    Returns:\n",
    "    - imputed_entries_store: dictionary containing all imputed entries\n",
    "    \"\"\"\n",
    "    imputed_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "    \n",
    "    for row, col in null_entries: # loop over every removed entry:\n",
    "        col_name = imputed_data.columns[col] # store column name of current null entry\n",
    "        if col_name not in imputed_entries_store: # check if column name is already in the dictionary\n",
    "            imputed_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "        imputed_entries_store[col_name].append(imputed_data.iloc[row, col])# add actual value to the list for correct column\n",
    "\n",
    "    return imputed_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41097e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": round(mean_squared_error(actual_vals, imputed_vals, squared=False),3)}) # add the column name and its RMSE\n",
    "    \n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d51bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": round(accuracy,3), \"F1 Score\": round(f1,3)}) # add the column name and its accuracy and F1 score\n",
    "    \n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab05fb",
   "metadata": {},
   "source": [
    "### Prepare Data for Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eff845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducability\n",
    "random.seed(24)\n",
    "\n",
    "# apply the add_missing_values function to our data\n",
    "X_miss, Y_miss, actual_entries_X = add_missing_values(X_copy, Y_copy, 0.05)\n",
    "\n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries_X = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(X_miss.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550dd8de",
   "metadata": {},
   "source": [
    "We have all the components we need in order to test out our different imputation methods:\n",
    "* random seed is set\n",
    "* X_miss and Y_miss can be duplicated and the imputation methods can be applied to each of the copies\n",
    "* actual_entries_X stores the original values from X_copy which have now been replaced by NaN\n",
    "* null_entries_X stores the locations of all NaN values in X_miss\n",
    "\n",
    "So now all we need to do for each imputation method is the following:\n",
    "* make a new copy of X_miss\n",
    "* apply the imputation algorithm to the copy\n",
    "* apply the get_imputed_values function to the resulting dataset\n",
    "* apply the calc_num_metrics and calc_categorical_metrics functions to assess the performance of the imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f883013",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(24) # set a random seed for reproducability\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "copy_raw_training_data = raw_training_data.copy()  # copy the raw training data\n",
    "shape = copy_raw_training_data.shape  # store dimension of raw training data\n",
    "num_entries = shape[0] * (shape[1] - 1)  # store number of entries for all independent variables\n",
    "num_null = int(num_entries * 0.01)  # make 1% of the data be null entries\n",
    "\n",
    "# randomly select entries and replace them with NaN, excluding the response variable\n",
    "for _ in range(num_null):\n",
    "    rand_row, rand_col = random.randint(0, shape[0] - 1) , random.randint(0, shape[1] - 2) # select a random row and column\n",
    "    copy_raw_training_data.iloc[rand_row, rand_col] = np.nan # store entry as nan\n",
    "    \n",
    "missing_raw_training_data = copy_raw_training_data.copy()\n",
    "    \n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(copy_raw_training_data.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)] \n",
    "\n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in copy_raw_training_data.select_dtypes(include=['object']).columns.tolist() if col != 'Status']\n",
    "num_cols = [x for x in copy_raw_training_data.columns.drop(['Status']) if x not in categorical_cols]\n",
    "\n",
    "# store the actual entries that are in the missing value locations as a dictionary\n",
    "actual_entries_store = get_imputed_values(raw_training_data, null_entries)\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# specify how to encode each categorical variable as an integer, and define a reverse of this encoding\n",
    "numerical_encode = {\n",
    "    \"Drug\": {\"Placebo\": 0, \"D-penicillamine\": 1}, \"Sex\": {\"F\": 0, \"M\": 1}, \"Ascites\": {\"N\": 0, \"Y\": 1},\n",
    "    \"Hepatomegaly\": {\"N\": 0, \"Y\": 1}, \"Spiders\": {\"N\": 0, \"Y\": 1}, \"Edema\": {\"N\": 0, \"Y\": 1, \"S\": 2}}\n",
    "\n",
    "reverse_numerical_encode = {feature: {value: key for key, value in encoding.items()}\n",
    "                            for feature, encoding in numerical_encode.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee9d92",
   "metadata": {},
   "source": [
    "### Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "     Alk_Phos 1726.848\n",
      "         SGOT   44.724\n",
      "       Copper   76.725\n",
      "    Platelets   85.359\n",
      "        Stage    0.882\n",
      "  Prothrombin    0.815\n",
      "  Cholesterol  193.877\n",
      "          Age 3617.757\n",
      "    Bilirubin    4.378\n",
      "Tryglicerides   63.538\n",
      "       N_Days 1138.424\n",
      "      Albumin    0.358\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.512     0.347\n",
      "         Sex     0.939     0.909\n",
      "     Ascites     0.945     0.919\n",
      "Hepatomegaly     0.550     0.390\n",
      "     Spiders     0.779     0.683\n",
      "       Edema     0.899     0.851\n"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "random.seed(24)\n",
    "\n",
    "# apply the add_missing_values function to our data\n",
    "X_copy_miss, Y_copy_miss, actual_entries_X = add_missing_values(X_copy, Y_copy, 0.05)\n",
    "\n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries_X = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(X_copy_miss.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "actual_entries_X = get_imputed_values(X_copy, null_entries_X) # redefine actual location of missing entries\n",
    "X_copy_miss2 = X_copy_miss.copy() # define the same data frame with missing entries to access later\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in X_copy_miss.columns: # loop over each column of the data set \n",
    "    col_data = X_copy_miss[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries_X if col_name == X_copy_miss.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# Display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed1_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac8806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 2: impute each missing value with the mean value from k nearest neighbours\n",
    "X_copy_miss = X_copy_miss2.replace(numerical_encode)\n",
    "\n",
    "imputer2 = KNNImputer(n_neighbors=2, weights=\"uniform\")    # create a KNN imputer\n",
    "data_imputed_knn_X = pd.DataFrame(imputer2.fit_transform(X=X_copy_miss, y=Y_copy_miss), columns=X_copy_miss.columns) # apply the imputer to the data with missing values\n",
    "\n",
    "# add the target back to the result\n",
    "data_imputed_knn = pd.concat([data_imputed_knn_X, Y_copy_miss], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf876bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed2_entries_store = get_imputed_values(data_imputed_knn, null_entries_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e89e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Variable     RMSE\n",
      "     Alk_Phos 1874.174\n",
      " Hepatomegaly    0.521\n",
      "         Drug    0.568\n",
      "         SGOT   42.066\n",
      "      Ascites    0.213\n",
      "       Copper   69.414\n",
      "    Platelets   99.137\n",
      "        Stage    1.102\n",
      "      Spiders    0.413\n",
      "  Prothrombin    0.879\n",
      "          Sex    0.263\n",
      "  Cholesterol  173.945\n",
      "          Age 4274.268\n",
      "        Edema    0.490\n",
      "    Bilirubin    3.821\n",
      "Tryglicerides   58.302\n",
      "       N_Days 1163.729\n",
      "      Albumin    0.395\n"
     ]
    }
   ],
   "source": [
    "knn_metrics = calc_num_metrics(get_imputed_values(X_copy.replace(numerical_encode), null_entries_X), imputed2_entries_store, data_imputed_knn.columns)\n",
    "print(knn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38ba3e0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [['D-penicillamine' 'Placebo' nan ... nan 'D-penicillamine'\n  'D-penicillamine']] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m rows_with_over1_missing \u001b[38;5;241m=\u001b[39m num_missing_raw_training_data[num_missing_raw_training_data\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# impute missing values using the median of the column in the rows with 2 or more missing values\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m median_vals \u001b[38;5;241m=\u001b[39m \u001b[43mX_copy_miss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# store median value for each column in missing data frame\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m rows_with_over1_missing\u001b[38;5;241m.\u001b[39miterrows(): \u001b[38;5;66;03m# loop over rows with more than 1 missing value:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     missing_indices \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39misnull() \u001b[38;5;66;03m# store the columns in current row with null entries\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:11693\u001b[0m, in \u001b[0;36mDataFrame.median\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11685\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(\n\u001b[1;32m  11687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11692\u001b[0m ):\n\u001b[0;32m> 11693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[1;32m  11695\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py:12428\u001b[0m, in \u001b[0;36mNDFrame.median\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(\n\u001b[1;32m  12422\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12423\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12426\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12429\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmedian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py:12374\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12370\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12372\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:11549\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11545\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11547\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11548\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11549\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11550\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/internals/managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/internals/blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:11468\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/nanops.py:787\u001b[0m, in \u001b[0;36mnanmedian\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    785\u001b[0m     inferred \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values)\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 787\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert [['D-penicillamine' 'Placebo' nan ... nan 'D-penicillamine'\n  'D-penicillamine']] to numeric"
     ]
    }
   ],
   "source": [
    "# Imputation 3: for numerical variables, implement linear regression using the non-null rows to determine the regression lines.\n",
    "# For rows with more 2 or 3 missing values, fill in 1 or 2, respecitvely, entries with the median for that column\n",
    "# For categorical variables, implement a decision tree classifier to impute the missing entries.\n",
    "X_copy_miss = X_copy_miss.replace(reverse_numerical_encode)\n",
    "\n",
    "np.random.seed(24)\n",
    "# Store the missing data frame for only numerical variables. Store another data frame by dropping the null rows.\n",
    "num_missing_raw_training_data = X_copy_miss.drop(categorical_cols, axis=1)\n",
    "linear_regres_df = num_missing_raw_training_data.dropna()\n",
    "\n",
    "# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\n",
    "regressions = {col: {'intercept': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).intercept_,\n",
    "                     'coeffs': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).coef_}\n",
    "               for col in linear_regres_df.columns}\n",
    "\n",
    "# identify the rows with 1 missing value, and more than 1 missing value\n",
    "rows_with_one_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) == 1]\n",
    "rows_with_over1_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) >= 2]\n",
    "\n",
    "# impute missing values using the median of the column in the rows with 2 or more missing values\n",
    "median_vals = X_copy_miss.median() # store median value for each column in missing data frame\n",
    "for idx, row in rows_with_over1_missing.iterrows(): # loop over rows with more than 1 missing value:\n",
    "    missing_indices = row.isnull() # store the columns in current row with null entries\n",
    "    fill_indices = np.random.choice(missing_indices[missing_indices].index, missing_indices.sum() - 1, replace=False) # sellect indicies to fill at random\n",
    "    rows_with_over1_missing.loc[idx, fill_indices] = median_vals[fill_indices] #fill these entries using median of the column\n",
    "\n",
    "# combine data frames such that each row only contains 1 missing value\n",
    "new_rows_with_one_missing = pd.concat([rows_with_over1_missing, rows_with_one_missing]).sort_index()\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# impute missing values using linear regression\n",
    "for idx, row in new_rows_with_one_missing.iterrows(): # loop over each row in this new data frame\n",
    "    for col in num_cols: # loop over numerical variables:\n",
    "        if pd.isna(row[col]): # if value in current cell is na:\n",
    "            row[col] = (regressions[col]['coeffs'] * row.dropna()).sum() + regressions[col]['intercept'] # calc the imputed value using linear regression coefficients and intercept\n",
    "\n",
    "# map to convert numerical variables in 'null_entries' to their names\n",
    "mapping = {0: 'N_Days', 2: 'Age', 8: 'Bilirubin', 9: 'Cholesterol', 10: 'Albumin', 11: 'Copper', 12: 'Alk_Phos', 13: 'SGOT', 14: 'Tryglicerides', 15: 'Platelets', 16: 'Prothrombin', 17: 'Stage'}\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# create the data set with numerical variables imputed as above and categorical variables still missing\n",
    "for index, row in new_rows_with_one_missing.iterrows():\n",
    "    for col in num_cols:\n",
    "        X_copy_miss.loc[index,col] = new_rows_with_one_missing.loc[index][col]\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Categorical imputation method:\n",
    "# split data set into numerical and categorical sets, dropping status column and all missing rows, and numerically encode\n",
    "dropped_missing_data = X_copy_miss.replace(numerical_encode).dropna()\n",
    "\n",
    "# train decision tree classifier for each categorical variable, predict the missing values, and impute into original dataset\n",
    "missing_training_data_encoded = X_copy_miss.replace(numerical_encode).copy() # new data frame by numerically encoding\n",
    "\n",
    "for cat_var in categorical_cols: # loop over each categorical variable\n",
    "    # define the training data and labels\n",
    "    X = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    y = dropped_missing_data[cat_var]\n",
    "    # fit the tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X, y)\n",
    "    # predict the missing categorical variables and impute into missing_raw_training_data\n",
    "    X_missing = X_copy_miss.drop(columns=categorical_cols)\n",
    "    missing_predictions = decision_tree.predict(X_missing)\n",
    "    X_copy_miss.loc[X_copy_miss[cat_var].isnull(), cat_var] = missing_predictions[X_copy_miss[cat_var].isnull()]\n",
    "# restore the original categorical entries instead of being numerical\n",
    "X_copy_miss = X_copy_miss.replace(reverse_numerical_encode)\n",
    "\n",
    "for cat_var in categorical_cols: # loop over each categorical variable\n",
    "    # define the training data and labels\n",
    "    X = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    y = dropped_missing_data[cat_var]\n",
    "    # fit the tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X, y)\n",
    "    # predict the missing categorical variables and impute into missing_raw_training_data\n",
    "    X_missing = X_copy_miss.drop(columns=categorical_cols)\n",
    "    missing_predictions = decision_tree.predict(X_missing)\n",
    "    X_copy_miss.loc[X_copy_miss[cat_var].isnull(), cat_var] = missing_predictions[X_copy_miss[cat_var].isnull()]\n",
    "# restore the original categorical entries instead of being numerical\n",
    "X_copy_miss = X_copy_miss.replace(reverse_numerical_encode)\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "imputed3_entries_store = get_imputed_values(X_copy_miss, null_entries_X) # store imputed values in dictionary\n",
    "print(\"Numerical variables performance for imputation 3:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed3_entries_store, num_cols))\n",
    "print(\"\\nNumerical variables performance for imputation 3:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed3_entries_store, categorical_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
