{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167bcedc",
   "metadata": {},
   "source": [
    "### Import Relevent Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/trf1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/trf1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# remove warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11061e76",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "\n",
    "# copy the raw training data\n",
    "copy_raw_training_data = raw_training_data.copy()\n",
    "\n",
    "# separate out the independent variables from the target\n",
    "X_copy = copy_raw_training_data.drop(labels=['Status'], axis=1)\n",
    "Y_copy = copy_raw_training_data[\"Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebdae6",
   "metadata": {},
   "source": [
    "### Define Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3cd36630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_values(X_full, Y_full, missing_rate):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and adds in some missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_full: a dataframe containing all columns with independent variables\n",
    "    - Y_full: a dataframe containing the response variable column only\n",
    "    - missing_rate: a float between 0 and 1 which specifies the proportion of lines which should have missing values\n",
    "\n",
    "    Returns:\n",
    "    - X_missing: the dataframe X_full but with some missing values\n",
    "    - Y_missing: an exact copy of Y_full\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "    np.random.seed(24)\n",
    "    \n",
    "    # copy the input dataframes\n",
    "    X_missing = X_full.copy()\n",
    "    y_missing = Y_full.copy()\n",
    "\n",
    "    # create empty dictionary to store values of entries before setting them to nan\n",
    "    actual_entries_store = {}\n",
    "\n",
    "    # remove a percentage of entries in each column at random, specified by missing_rate\n",
    "    for col in X_missing.columns:\n",
    "        index_list = X_missing.sample(frac=missing_rate+random.uniform(-0.01, 0.01)).index  # sample a proportion of the indices within the column, with a random component for the rate\n",
    "        actual_entries_store[col] = []\n",
    "        for value in index_list:\n",
    "            actual_entries_store[col].append(X_missing.loc[value, col])   # store the actual entries in the dictionary\n",
    "        X_missing.loc[index_list, col] = np.nan # replace the values with nan\n",
    "\n",
    "    return X_missing, y_missing, actual_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "574af618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_values(imputed_data, null_entries):\n",
    "    \"\"\"\n",
    "    Gets the value of all the imputed entries and outputs them in a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - imputed_data: the dataframe containing some actual values and some imputed values\n",
    "    - null_entries: the locations of all the entries which were null before imputation\n",
    "\n",
    "    Returns:\n",
    "    - imputed_entries_store: dictionary containing all imputed entries\n",
    "    \"\"\"\n",
    "    imputed_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "    \n",
    "    for row, col in null_entries: # loop over every removed entry:\n",
    "        col_name = imputed_data.columns[col] # store column name of current null entry\n",
    "        if col_name not in imputed_entries_store: # check if column name is already in the dictionary\n",
    "            imputed_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "        imputed_entries_store[col_name].append(imputed_data.iloc[row, col])# add actual value to the list for correct column\n",
    "\n",
    "    return imputed_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "41097e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": round(mean_squared_error(actual_vals, imputed_vals, squared=False),3)}) # add the column name and its RMSE\n",
    "    \n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4d51bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": round(accuracy,3), \"F1 Score\": round(f1,3)}) # add the column name and its accuracy and F1 score\n",
    "    \n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2de12964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_encode(X):\n",
    "    \"\"\"\n",
    "    Encodes the categorical variables in a given dataframe numerically.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: dataframe, should have columns labelled \"Drug\", \"Sex\", \"Ascites\", \"Hepatomegaly\", \"Spiders\", \"Edema\"\n",
    "    \n",
    "    Returns:\n",
    "    - X_encoded: dataframe with categorical variables encoded numerically\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the dataframe\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "    # specify how each numerical column should be encoded\n",
    "    encodings = {\n",
    "        \"Drug\": {\"Placebo\": 0, \"D-penicillamine\": 1},\n",
    "        \"Sex\": {\"F\": 0, \"M\": 1},\n",
    "        \"Ascites\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Hepatomegaly\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Spiders\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Edema\": {\"N\": 0, \"Y\": 1, \"S\": 2}\n",
    "    }\n",
    "\n",
    "    # replace the values in categorical columns by their encoded values\n",
    "    X_encoded = X_encoded.replace(encodings)\n",
    "\n",
    "    # output the encoded dataframe\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0b97fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_numerical_encode(X):\n",
    "    \"\"\"\n",
    "    Reverses the numerical encoding process by replacing numerical values in categorical columns with the closest category.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: dataframe which has its categorical columns encoded\n",
    "    \n",
    "    Returns:\n",
    "    - X_decoded: dataframe with categorical columns decoded\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the input dataframe\n",
    "    X_decoded = X.copy()\n",
    "\n",
    "    # specify the inverse of the encoding\n",
    "    decodings = {\n",
    "        \"Drug\": {0: \"Placebo\", 1: \"D-penicillamine\"},\n",
    "        \"Sex\": {0: \"F\", 1: \"M\"},\n",
    "        \"Ascites\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Hepatomegaly\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Spiders\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Edema\": {0: \"N\", 1: \"Y\", 2: \"S\"}\n",
    "    }\n",
    "\n",
    "    # replace each categorical column with numbers closest to the ones in the list above\n",
    "    for col in decodings:\n",
    "        new_col = []                            # stores a replacement column with numbers that correspond to decodings\n",
    "        for entry in X_decoded[col]:\n",
    "            closest_val = 0                     # stores the closest value so far, returns 0 if entry is nan\n",
    "            min_dist = 100                      # stores highscore of minimum distance, starts very high to ensure it's overtaken\n",
    "            for option in decodings[col]:\n",
    "                dist = abs(option - entry)      # calculate the distance of the given entry to each option\n",
    "                if dist < min_dist:\n",
    "                    closest_val = option        # updates closest value and minimum distance if it's a new highscore\n",
    "                    min_dist = dist\n",
    "            new_col.append(closest_val)         # adds the final closest value to the new column\n",
    "        new_col_df = pd.DataFrame(new_col)      # makes the replacement column into a dataframe and replaces the old column with it\n",
    "        X_decoded[col] = new_col_df\n",
    "\n",
    "    # apply the reverse encoding now that all entries are options for categories\n",
    "    X_decoded = X_decoded.replace(decodings)\n",
    "\n",
    "    # output the decoded dataframe\n",
    "    return X_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab05fb",
   "metadata": {},
   "source": [
    "### Prepare Data for Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9eff845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducability\n",
    "random.seed(24)\n",
    "\n",
    "# apply the add_missing_values function to our data\n",
    "X_miss, Y_miss, actual_entries_X = add_missing_values(X_copy, Y_copy, 0.05)\n",
    "\n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries_X = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(X_miss.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in X_miss.select_dtypes(include=['object']).columns.tolist()]\n",
    "num_cols = [x for x in X_miss.columns if x not in categorical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550dd8de",
   "metadata": {},
   "source": [
    "We have all the components we need in order to test out our different imputation methods:\n",
    "* random seed is set\n",
    "* X_miss and Y_miss can be duplicated and the imputation methods can be applied to each of the copies\n",
    "* actual_entries_X stores the original values from X_copy which have now been replaced by NaN\n",
    "* null_entries_X stores the locations of all NaN values in X_miss\n",
    "* categorical_cols and num_cols store the names of categorical and numerical columns respectively\n",
    "\n",
    "So now all we need to do for each imputation method is the following:\n",
    "* make a new copy of X_miss\n",
    "* apply the imputation algorithm to the copy\n",
    "* apply the get_imputed_values function to the resulting dataset\n",
    "* apply the calc_num_metrics and calc_categorical_metrics functions to assess the performance of the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee9d92",
   "metadata": {},
   "source": [
    "### Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1138.424\n",
      "          Age 3617.757\n",
      "    Bilirubin    4.378\n",
      "  Cholesterol  193.877\n",
      "      Albumin    0.358\n",
      "       Copper   76.725\n",
      "     Alk_Phos 1726.848\n",
      "         SGOT   44.724\n",
      "Tryglicerides   63.538\n",
      "    Platelets   85.359\n",
      "  Prothrombin    0.815\n",
      "        Stage    0.882\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.512     0.347\n",
      "         Sex     0.939     0.909\n",
      "     Ascites     0.945     0.919\n",
      "Hepatomegaly     0.550     0.390\n",
      "     Spiders     0.779     0.683\n",
      "       Edema     0.899     0.851\n"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy1 = X_miss.copy()\n",
    "\n",
    "# apply the imputation method\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in X_miss_copy1.columns: # loop over each column of the data set \n",
    "    col_data = X_miss_copy1[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries_X if col_name == X_miss_copy1.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed1_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9ac8806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 2: impute each missing value with the mean value from k nearest neighbours\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy2 = X_miss.copy()\n",
    "\n",
    "# encode the categorical data to be numerical\n",
    "X_miss_copy2 = numerical_encode(X_miss_copy2)\n",
    "\n",
    "# create a KNN imputer and apply it to this copy of the data\n",
    "imputer2 = KNNImputer(n_neighbors=20, weights=\"uniform\")    # create a KNN imputer\n",
    "X_miss_copy2 = pd.DataFrame(imputer2.fit_transform(X=X_miss_copy2, y=Y_miss), columns=X_miss_copy2.columns) # apply the imputer to the data with missing values\n",
    "\n",
    "# reverse the numerical encoding of the categorical data\n",
    "X_miss_copy2 = reverse_numerical_encode(X_miss_copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cf876bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the imputed values from this imputation method and store them\n",
    "imputed2_entries_store = get_imputed_values(X_miss_copy2, null_entries_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0e89e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1219.340\n",
      "          Age 3748.735\n",
      "    Bilirubin    4.492\n",
      "  Cholesterol  205.170\n",
      "      Albumin    0.374\n",
      "       Copper   78.695\n",
      "     Alk_Phos 1717.350\n",
      "         SGOT   48.352\n",
      "Tryglicerides   65.929\n",
      "    Platelets   89.474\n",
      "  Prothrombin    0.859\n",
      "        Stage    0.929\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.539     0.523\n",
      "         Sex     0.932     0.906\n",
      "     Ascites     0.934     0.913\n",
      "Hepatomegaly     0.472     0.473\n",
      "     Spiders     0.773     0.695\n",
      "       Edema     0.881     0.842\n"
     ]
    }
   ],
   "source": [
    "# display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed2_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed2_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "38ba3e0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5739 is out of bounds for axis 0 with size 5739",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1064792/560871446.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;31m# Imputation 3: for numerical variables, implement linear regression using the non-null rows to determine the regression lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m# For rows with more 2 or 3 missing values, fill in 1 or 2, respecitvely, entries with the median for that column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# For categorical variables, implement a decision tree classifier to impute the missing entries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1064792/1654495430.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(imputed_data, null_entries)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnull_entries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loop over every removed entry:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcol_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# store column name of current null entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimputed_entries_store\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# check if column name is already in the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mimputed_entries_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# if not already in dictionary, create empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mimputed_entries_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# add actual value to the list for correct column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimputed_entries_store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4196\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mCaller\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mresponsible\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchecking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \"\"\"\n\u001b[1;32m   4198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m             \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4203\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5739 is out of bounds for axis 0 with size 5739"
     ]
    }
   ],
   "source": [
    "# Imputation 3: for numerical variables, implement linear regression using the non-null rows to determine the regression lines.\n",
    "# For rows with more 2 or 3 missing values, fill in 1 or 2, respecitvely, entries with the median for that column\n",
    "# For categorical variables, implement a decision tree classifier to impute the missing entries.\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy3 = X_miss.copy()\n",
    "\n",
    "np.random.seed(24)\n",
    "# Store the missing data frame for only numerical variables. Store another data frame by dropping the null rows.\n",
    "num_missing_raw_training_data = X_miss_copy3.drop(categorical_cols, axis=1)\n",
    "linear_regres_df = num_missing_raw_training_data.dropna()\n",
    "\n",
    "# identify the rows with 1 missing value, and more than 1 missing value\n",
    "rows_with_one_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) == 1]\n",
    "rows_with_over1_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) >= 2]\n",
    "\n",
    "# impute missing values using the median of the column in the rows with 2 or more missing values\n",
    "median_vals = num_missing_raw_training_data.median() # store median value for each column in missing data frame\n",
    "for idx, row in rows_with_over1_missing.iterrows(): # loop over rows with more than 1 missing value:\n",
    "    missing_indices = row.isnull() # store the columns in current row with null entries\n",
    "    fill_indices = np.random.choice(missing_indices[missing_indices].index, missing_indices.sum() - 1, replace=False) # sellect indicies to fill at random\n",
    "    rows_with_over1_missing.loc[idx, fill_indices] = median_vals[fill_indices] #fill these entries using median of the column\n",
    "\n",
    "# combine data frames such that each row only contains 1 missing value\n",
    "new_rows_with_one_missing = pd.concat([rows_with_over1_missing, rows_with_one_missing]).sort_index()\n",
    "\n",
    "# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\n",
    "regressions = {col: {'intercept': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).intercept_,\n",
    "                     'coeffs': LinearRegression().fit(linear_regres_df.drop(col, axis=1),linear_regres_df[col]).coef_}\n",
    "               for col in linear_regres_df.columns}\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# impute missing values using linear regression\n",
    "for idx, row in new_rows_with_one_missing.iterrows(): # loop over each row in this new data frame\n",
    "    for col in num_cols: # loop over numerical variables:\n",
    "        if pd.isna(row[col]): # if value in current cell is na:\n",
    "            row[col] = (regressions[col]['coeffs'] * row.dropna()).sum() + regressions[col]['intercept'] # calc the imputed value using linear regression coefficients and intercept\n",
    "\n",
    "# map to convert numerical variables in 'null_entries' to their names\n",
    "mapping = {0: 'N_Days', 2: 'Age', 8: 'Bilirubin', 9: 'Cholesterol', 10: 'Albumin', 11: 'Copper', 12: 'Alk_Phos', 13: 'SGOT', 14: 'Tryglicerides', 15: 'Platelets', 16: 'Prothrombin', 17: 'Stage'}\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# create the data set with numerical variables imputed as above and categorical variables still missing\n",
    "for index, row in new_rows_with_one_missing.iterrows():\n",
    "    for col in num_cols:\n",
    "        X_miss_copy3.loc[index,col] = new_rows_with_one_missing.loc[index][col]\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Categorical imputation method:\n",
    "# split data set into numerical and categorical sets, dropping status column and all missing rows, and numerically encode\n",
    "dropped_missing_data = numerical_encode(X_miss_copy3).dropna()\n",
    "\n",
    "for cat_var in categorical_cols: # loop over each categorical variable\n",
    "    # define the training data and labels\n",
    "    X = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    y = dropped_missing_data[cat_var]\n",
    "    # fit the tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X, y)\n",
    "    # predict the missing categorical variables and impute into missing_raw_training_data\n",
    "    X_missing = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    missing_predictions = decision_tree.predict(X_missing)\n",
    "    dropped_missing_data.loc[dropped_missing_data[cat_var].isnull(), cat_var] = missing_predictions[dropped_missing_data[cat_var].isnull()]\n",
    "# restore the original categorical entries instead of being numerical\n",
    "X_miss_copy3 = reverse_numerical_encode(dropped_missing_data)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "imputed3_entries_store = get_imputed_values(X_miss_copy3, null_entries_X) # store imputed values in dictionary\n",
    "print(\"Numerical variables performance for imputation 3:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed3_entries_store, num_cols))\n",
    "print(\"\\nNumerical variables performance for imputation 3:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed3_entries_store, categorical_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
