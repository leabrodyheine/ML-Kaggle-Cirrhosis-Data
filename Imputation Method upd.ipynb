{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167bcedc",
   "metadata": {},
   "source": [
    "### Import Relevent Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0ae2d441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /home/trf1/.local/lib/python3.9/site-packages\n",
      "sysconfig: /home/trf1/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use pip to install all the libraries we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib scikit-learn seaborn | grep -v 'already satisfied'\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# remove warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11061e76",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8896b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training data, dropping the ID column, as this is duplicated by pandas\n",
    "raw_training_data = pd.read_csv(r\"train.csv\").drop(columns=[\"id\"])\n",
    "\n",
    "# copy the raw training data\n",
    "copy_raw_training_data = raw_training_data.copy()\n",
    "\n",
    "# separate out the independent variables from the target\n",
    "X_copy = copy_raw_training_data.drop(labels=['Status'], axis=1)\n",
    "Y_copy = copy_raw_training_data[\"Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebdae6",
   "metadata": {},
   "source": [
    "### Define Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3cd36630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_values(X_full, Y_full, missing_rate):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and adds in some missing values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_full: a dataframe containing all columns with independent variables\n",
    "    - Y_full: a dataframe containing the response variable column only\n",
    "    - missing_rate: a float between 0 and 1 which specifies the proportion of lines which should have missing values\n",
    "\n",
    "    Returns:\n",
    "    - X_missing: the dataframe X_full but with some missing values\n",
    "    - Y_missing: an exact copy of Y_full\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    \"\"\"\n",
    "    # set a seed for reproducibility\n",
    "    np.random.seed(24)\n",
    "    \n",
    "    # copy the input dataframes\n",
    "    X_missing = X_full.copy()\n",
    "    y_missing = Y_full.copy()\n",
    "\n",
    "    # create empty dictionary to store values of entries before setting them to nan\n",
    "    actual_entries_store = {}\n",
    "\n",
    "    # remove a percentage of entries in each column at random, specified by missing_rate\n",
    "    for col in X_missing.columns:\n",
    "        index_list = X_missing.sample(frac=missing_rate+random.uniform(-0.01, 0.01)).index  # sample a proportion of the indices within the column, with a random component for the rate\n",
    "        actual_entries_store[col] = []\n",
    "        for value in index_list:\n",
    "            actual_entries_store[col].append(X_missing.loc[value, col])   # store the actual entries in the dictionary\n",
    "        X_missing.loc[index_list, col] = np.nan # replace the values with nan\n",
    "\n",
    "    return X_missing, y_missing, actual_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "574af618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_values(imputed_data, null_entries):\n",
    "    \"\"\"\n",
    "    Gets the value of all the imputed entries and outputs them in a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - imputed_data: the dataframe containing some actual values and some imputed values\n",
    "    - null_entries: the locations of all the entries which were null before imputation\n",
    "\n",
    "    Returns:\n",
    "    - imputed_entries_store: dictionary containing all imputed entries\n",
    "    \"\"\"\n",
    "    imputed_entries_store = {} # create dictionary for storing the actual values in the locations that are removed\n",
    "    \n",
    "    for row, col in null_entries: # loop over every removed entry:\n",
    "        col_name = imputed_data.columns[col] # store column name of current null entry\n",
    "        if col_name not in imputed_entries_store: # check if column name is already in the dictionary\n",
    "            imputed_entries_store[col_name] = [] # if not already in dictionary, create empty list\n",
    "        imputed_entries_store[col_name].append(imputed_data.iloc[row, col])# add actual value to the list for correct column\n",
    "\n",
    "    return imputed_entries_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "41097e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_metrics(actual_entries_store, imputed_entries_store, num_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric RMSE for numerical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - num_cols: list of the numerical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the RMSE for the imputation method for each numerical variable\n",
    "    \"\"\"\n",
    "    numerical_data = []\n",
    "    for col_name, actual_vals in actual_entries_store.items(): # loop over all variables and actual values\n",
    "        if col_name in num_cols: # if the current column is numerical:\n",
    "            # change imputed and actual values to be numeric\n",
    "            imputed_vals = pd.to_numeric(imputed_entries_store.get(col_name, []))\n",
    "            actual_vals = pd.to_numeric(actual_vals)\n",
    "            numerical_data.append({\"Variable\": col_name, \"RMSE\": round(mean_squared_error(actual_vals, imputed_vals, squared=False),3)}) # add the column name and its RMSE\n",
    "    \n",
    "    return pd.DataFrame(numerical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4d51bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_categorical_metrics(actual_entries_store, imputed_entries_store, categorical_cols):\n",
    "    \"\"\"\n",
    "    Calculates performance metric accuracy and F1 score for categorical variables for the imputation method.\n",
    "    \n",
    "    Parameters:\n",
    "    - actual_entries_store: dictionary containing all entries that were set to NaN, keys are the data frames columns\n",
    "    - imputed_entries_store: dictionary containing all imputed entries, indexed the same as actual_entries_store\n",
    "    - categorical_cols: list of the categorical columns in the data frame\n",
    "    \n",
    "    Returns:\n",
    "    - Data frame as a string showing the accuracy and F1 score for the imputation method for each categorical variable\n",
    "    \"\"\"\n",
    "    categorical_data = []\n",
    "    for col_name in categorical_cols: # loop over all categorical variables\n",
    "        # change imputed and actual values to be a string\n",
    "        imputed_vals = [str(val) for val in imputed_entries_store.get(col_name, [])]\n",
    "        actual_vals = [str(val) for val in actual_entries_store[col_name]]\n",
    "        accuracy = accuracy_score(actual_vals, imputed_vals) # calculate accuracy score\n",
    "        f1 = f1_score(actual_vals, imputed_vals, average='weighted') # calculate F1 score\n",
    "        categorical_data.append({\"Variable\": col_name, \"Accuracy\": round(accuracy,3), \"F1 Score\": round(f1,3)}) # add the column name and its accuracy and F1 score\n",
    "    \n",
    "    return pd.DataFrame(categorical_data).to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2de12964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_encode(X):\n",
    "    \"\"\"\n",
    "    Encodes the categorical variables in a given dataframe numerically.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: dataframe, should have columns labelled \"Drug\", \"Sex\", \"Ascites\", \"Hepatomegaly\", \"Spiders\", \"Edema\"\n",
    "    \n",
    "    Returns:\n",
    "    - X_encoded: dataframe with categorical variables encoded numerically\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the dataframe\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "    # specify how each numerical column should be encoded\n",
    "    encodings = {\n",
    "        \"Drug\": {\"Placebo\": 0, \"D-penicillamine\": 1},\n",
    "        \"Sex\": {\"F\": 0, \"M\": 1},\n",
    "        \"Ascites\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Hepatomegaly\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Spiders\": {\"N\": 0, \"Y\": 1},\n",
    "        \"Edema\": {\"N\": 0, \"Y\": 1, \"S\": 2}\n",
    "    }\n",
    "\n",
    "    # replace the values in categorical columns by their encoded values\n",
    "    X_encoded = X_encoded.replace(encodings)\n",
    "\n",
    "    # output the encoded dataframe\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0b97fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_numerical_encode(X):\n",
    "    \"\"\"\n",
    "    Reverses the numerical encoding process by replacing numerical values in categorical columns with the closest category.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: dataframe which has its categorical columns encoded\n",
    "    \n",
    "    Returns:\n",
    "    - X_decoded: dataframe with categorical columns decoded\n",
    "    \"\"\"\n",
    "\n",
    "    # make a copy of the input dataframe\n",
    "    X_decoded = X.copy()\n",
    "\n",
    "    # specify the inverse of the encoding\n",
    "    decodings = {\n",
    "        \"Drug\": {0: \"Placebo\", 1: \"D-penicillamine\"},\n",
    "        \"Sex\": {0: \"F\", 1: \"M\"},\n",
    "        \"Ascites\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Hepatomegaly\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Spiders\": {0: \"N\", 1: \"Y\"},\n",
    "        \"Edema\": {0: \"N\", 1: \"Y\", 2: \"S\"}\n",
    "    }\n",
    "\n",
    "    # replace each categorical column with numbers closest to the ones in the list above\n",
    "    for col in decodings:\n",
    "        new_col = []                            # stores a replacement column with numbers that correspond to decodings\n",
    "        for entry in X_decoded[col]:\n",
    "            closest_val = 0                     # stores the closest value so far, returns 0 if entry is nan\n",
    "            min_dist = 100                      # stores highscore of minimum distance, starts very high to ensure it's overtaken\n",
    "            for option in decodings[col]:\n",
    "                dist = abs(option - entry)      # calculate the distance of the given entry to each option\n",
    "                if dist < min_dist:\n",
    "                    closest_val = option        # updates closest value and minimum distance if it's a new highscore\n",
    "                    min_dist = dist\n",
    "            new_col.append(closest_val)         # adds the final closest value to the new column\n",
    "        new_col_df = pd.DataFrame(new_col)      # makes the replacement column into a dataframe and replaces the old column with it\n",
    "        X_decoded[col] = new_col_df\n",
    "\n",
    "    # apply the reverse encoding now that all entries are options for categories\n",
    "    X_decoded = X_decoded.replace(decodings)\n",
    "\n",
    "    # output the decoded dataframe\n",
    "    return X_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab05fb",
   "metadata": {},
   "source": [
    "### Prepare Data for Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9eff845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducability\n",
    "random.seed(24)\n",
    "\n",
    "# apply the add_missing_values function to our data\n",
    "X_miss, Y_miss, actual_entries_X = add_missing_values(X_copy, Y_copy, 0.05)\n",
    "\n",
    "# create a list to store all of the entries which are null: (row, column)\n",
    "null_entries_X = [(row_index, col_index) \n",
    "                for row_index, row in enumerate(X_miss.values) \n",
    "                for col_index, val in enumerate(row) \n",
    "                if pd.isnull(val)]\n",
    "\n",
    "# store the names of all categorical and numerical columns (excluding the response variable)\n",
    "categorical_cols = [col for col in X_miss.select_dtypes(include=['object']).columns.tolist()]\n",
    "num_cols = [x for x in X_miss.columns if x not in categorical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550dd8de",
   "metadata": {},
   "source": [
    "We have all the components we need in order to test out our different imputation methods:\n",
    "* random seed is set\n",
    "* X_miss and Y_miss can be duplicated and the imputation methods can be applied to each of the copies\n",
    "* actual_entries_X stores the original values from X_copy which have now been replaced by NaN\n",
    "* null_entries_X stores the locations of all NaN values in X_miss\n",
    "* categorical_cols and num_cols store the names of categorical and numerical columns respectively\n",
    "\n",
    "So now all we need to do for each imputation method is the following:\n",
    "* make a new copy of X_miss\n",
    "* apply the imputation algorithm to the copy\n",
    "* apply the get_imputed_values function to the resulting dataset\n",
    "* apply the calc_num_metrics and calc_categorical_metrics functions to assess the performance of the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee9d92",
   "metadata": {},
   "source": [
    "### Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "25e1663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1138.424\n",
      "          Age 3617.757\n",
      "    Bilirubin    4.378\n",
      "  Cholesterol  193.877\n",
      "      Albumin    0.358\n",
      "       Copper   76.725\n",
      "     Alk_Phos 1726.848\n",
      "         SGOT   44.724\n",
      "Tryglicerides   63.538\n",
      "    Platelets   85.359\n",
      "  Prothrombin    0.815\n",
      "        Stage    0.882\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.512     0.347\n",
      "         Sex     0.939     0.909\n",
      "     Ascites     0.945     0.919\n",
      "Hepatomegaly     0.550     0.390\n",
      "     Spiders     0.779     0.683\n",
      "       Edema     0.899     0.851\n"
     ]
    }
   ],
   "source": [
    "# Imputation 1: impute missing values with median for numerical variables and mode for categorical variables\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy1 = X_miss.copy()\n",
    "\n",
    "# apply the imputation method\n",
    "imputed1_entries_store = {} # create empty dictionary\n",
    "for col_name in X_miss_copy1.columns: # loop over each column of the data set \n",
    "    col_data = X_miss_copy1[col_name] # store data for current column\n",
    "    if col_data.dtype in ['float64', 'int64']: # check if data type of current column is numeric\n",
    "        col_data.fillna(col_data.median(), inplace=True) # impute numerical nulls with median\n",
    "    else:\n",
    "        col_data.fillna(col_data.mode().iloc[0], inplace=True) # impute categorical nulls with mode\n",
    "    imputed1_entries_store[col_name] = [col_data.loc[row] for row, col in null_entries_X if col_name == X_miss_copy1.columns[col]] # store the imputed values in the dictionary\n",
    "\n",
    "# display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed1_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed1_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9ac8806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation 2: impute each missing value with the mean value from k nearest neighbours\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy2 = X_miss.copy()\n",
    "\n",
    "# encode the categorical data to be numerical\n",
    "X_miss_copy2 = numerical_encode(X_miss_copy2)\n",
    "\n",
    "# create a KNN imputer and apply it to this copy of the data\n",
    "imputer2 = KNNImputer(n_neighbors=20, weights=\"uniform\")    # create a KNN imputer\n",
    "X_miss_copy2 = pd.DataFrame(imputer2.fit_transform(X=X_miss_copy2, y=Y_miss), columns=X_miss_copy2.columns) # apply the imputer to the data with missing values\n",
    "\n",
    "# reverse the numerical encoding of the categorical data\n",
    "X_miss_copy2 = reverse_numerical_encode(X_miss_copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cf876bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the imputed values from this imputation method and store them\n",
    "imputed2_entries_store = get_imputed_values(X_miss_copy2, null_entries_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0e89e0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables performance for imputation 1:\n",
      "     Variable     RMSE\n",
      "       N_Days 1219.340\n",
      "          Age 3748.735\n",
      "    Bilirubin    4.492\n",
      "  Cholesterol  205.170\n",
      "      Albumin    0.374\n",
      "       Copper   78.695\n",
      "     Alk_Phos 1717.350\n",
      "         SGOT   48.352\n",
      "Tryglicerides   65.929\n",
      "    Platelets   89.474\n",
      "  Prothrombin    0.859\n",
      "        Stage    0.929\n",
      "\n",
      "Categorical variables performance for imputation 1:\n",
      "    Variable  Accuracy  F1 Score\n",
      "        Drug     0.539     0.523\n",
      "         Sex     0.932     0.906\n",
      "     Ascites     0.934     0.913\n",
      "Hepatomegaly     0.472     0.473\n",
      "     Spiders     0.773     0.695\n",
      "       Edema     0.881     0.842\n"
     ]
    }
   ],
   "source": [
    "# display performance metrics\n",
    "print(\"Numerical variables performance for imputation 1:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed2_entries_store, num_cols))\n",
    "print(\"\\nCategorical variables performance for imputation 1:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed2_entries_store, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "38ba3e0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[240], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m new_rows_with_one_missing \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([rows_with_over1_missing, rows_with_one_missing])\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m regressions \u001b[38;5;241m=\u001b[39m {col: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearRegression()\u001b[38;5;241m.\u001b[39mfit(new_rows_with_one_missing\u001b[38;5;241m.\u001b[39mdrop(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),new_rows_with_one_missing[col])\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoeffs\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearRegression()\u001b[38;5;241m.\u001b[39mfit(new_rows_with_one_missing\u001b[38;5;241m.\u001b[39mdrop(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),new_rows_with_one_missing[col])\u001b[38;5;241m.\u001b[39mcoef_}\n\u001b[1;32m     30\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m new_rows_with_one_missing\u001b[38;5;241m.\u001b[39mcolumns}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# impute missing values using linear regression\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m new_rows_with_one_missing\u001b[38;5;241m.\u001b[39miterrows(): \u001b[38;5;66;03m# loop over each row in this new data frame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[240], line 28\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m new_rows_with_one_missing \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([rows_with_over1_missing, rows_with_one_missing])\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m regressions \u001b[38;5;241m=\u001b[39m {col: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_rows_with_one_missing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_rows_with_one_missing\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoeffs\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearRegression()\u001b[38;5;241m.\u001b[39mfit(new_rows_with_one_missing\u001b[38;5;241m.\u001b[39mdrop(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),new_rows_with_one_missing[col])\u001b[38;5;241m.\u001b[39mcoef_}\n\u001b[1;32m     30\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m new_rows_with_one_missing\u001b[38;5;241m.\u001b[39mcolumns}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# impute missing values using linear regression\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m new_rows_with_one_missing\u001b[38;5;241m.\u001b[39miterrows(): \u001b[38;5;66;03m# loop over each row in this new data frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/linear_model/_base.py:578\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    574\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    576\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 578\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Imputation 3: for numerical variables, implement linear regression using the non-null rows to determine the regression lines.\n",
    "# For rows with more 2 or 3 missing values, fill in 1 or 2, respecitvely, entries with the median for that column\n",
    "# For categorical variables, implement a decision tree classifier to impute the missing entries.\n",
    "\n",
    "# make a copy of the data with missing values\n",
    "X_miss_copy3 = X_miss.copy()\n",
    "\n",
    "np.random.seed(24)\n",
    "# Store the missing data frame for only numerical variables. Store another data frame by dropping the null rows.\n",
    "num_missing_raw_training_data = X_miss_copy3.drop(categorical_cols, axis=1)\n",
    "#linear_regres_df = num_missing_raw_training_data\n",
    "\n",
    "# identify the rows with 1 missing value, and more than 1 missing value\n",
    "rows_with_one_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) == 1]\n",
    "rows_with_over1_missing = num_missing_raw_training_data[num_missing_raw_training_data.isnull().sum(axis=1) >= 2]\n",
    "\n",
    "# impute missing values using the median of the column in the rows with 2 or more missing values\n",
    "median_vals = num_missing_raw_training_data.median() # store median value for each column in missing data frame\n",
    "for idx, row in rows_with_over1_missing.iterrows(): # loop over rows with more than 1 missing value:\n",
    "    missing_indices = row.isnull() # store the columns in current row with null entries\n",
    "    fill_indices = np.random.choice(missing_indices[missing_indices].index, missing_indices.sum() - 1, replace=False) # sellect indicies to fill at random\n",
    "    rows_with_over1_missing.loc[idx, fill_indices] = median_vals[fill_indices] #fill these entries using median of the column\n",
    "\n",
    "# combine data frames such that each row only contains 1 missing value\n",
    "new_rows_with_one_missing = pd.concat([rows_with_over1_missing, rows_with_one_missing]).sort_index()\n",
    "\n",
    "# fit linear regression models (for each numerical variable as the response) and store the coefficients and intercepts\n",
    "regressions = {col: {'intercept': LinearRegression().fit(new_rows_with_one_missing.drop(col, axis=1),new_rows_with_one_missing[col]).intercept_,\n",
    "                     'coeffs': LinearRegression().fit(new_rows_with_one_missing.drop(col, axis=1),new_rows_with_one_missing[col]).coef_}\n",
    "               for col in new_rows_with_one_missing.columns}\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# impute missing values using linear regression\n",
    "for idx, row in new_rows_with_one_missing.iterrows(): # loop over each row in this new data frame\n",
    "    for col in num_cols: # loop over numerical variables:\n",
    "        if pd.isna(row[col]): # if value in current cell is na:\n",
    "            row[col] = (regressions[col]['coeffs'] * row.dropna()).sum() + regressions[col]['intercept'] # calc the imputed value using linear regression coefficients and intercept\n",
    "\n",
    "# map to convert numerical variables in 'null_entries' to their names\n",
    "mapping = {0: 'N_Days', 2: 'Age', 8: 'Bilirubin', 9: 'Cholesterol', 10: 'Albumin', 11: 'Copper', 12: 'Alk_Phos', 13: 'SGOT', 14: 'Tryglicerides', 15: 'Platelets', 16: 'Prothrombin', 17: 'Stage'}\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# create the data set with numerical variables imputed as above and categorical variables still missing\n",
    "for index, row in new_rows_with_one_missing.iterrows():\n",
    "    for col in num_cols:\n",
    "        X_miss_copy3.loc[index,col] = new_rows_with_one_missing.loc[index][col]\n",
    "        \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Categorical imputation method:\n",
    "# split data set into numerical and categorical sets, dropping status column and all missing rows, and numerically encode\n",
    "dropped_missing_data = numerical_encode(X_miss_copy3).dropna()\n",
    "\n",
    "for cat_var in categorical_cols: # loop over each categorical variable\n",
    "    # define the training data and labels\n",
    "    X = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    y = dropped_missing_data[cat_var]\n",
    "    # fit the tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X, y)\n",
    "    # predict the missing categorical variables and impute into missing_raw_training_data\n",
    "    X_missing = dropped_missing_data.drop(columns=categorical_cols)\n",
    "    missing_predictions = decision_tree.predict(X_missing)\n",
    "    dropped_missing_data.loc[dropped_missing_data[cat_var].isnull(), cat_var] = missing_predictions[dropped_missing_data[cat_var].isnull()]\n",
    "# restore the original categorical entries instead of being numerical\n",
    "X_miss_copy3 = reverse_numerical_encode(dropped_missing_data)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "imputed3_entries_store = get_imputed_values(X_miss_copy3, null_entries_X) # store imputed values in dictionary\n",
    "print(\"Numerical variables performance for imputation 3:\")\n",
    "print(calc_num_metrics(actual_entries_X, imputed3_entries_store, num_cols))\n",
    "print(\"\\nNumerical variables performance for imputation 3:\")\n",
    "print(calc_categorical_metrics(actual_entries_X, imputed3_entries_store, categorical_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
